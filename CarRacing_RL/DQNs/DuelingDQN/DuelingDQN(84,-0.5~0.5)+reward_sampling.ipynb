{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOeFJI4xFg/C5bZdeSjQ2Zh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# import 必要套件"],"metadata":{"id":"wyMye7DSnBGR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"PeLozgNMm6Qf"},"outputs":[],"source":["import random\n","import time,math\n","import numpy as np\n","import gymnasium as gym\n","import gymnasium.wrappers as gym_wrap\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation #輸出動畫影片\n","from IPython import display\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn.functional as F\n","import collections\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","source":["print(device)"],"metadata":{"id":"ct98VnG3nFbX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 建立Replay Buffer類別"],"metadata":{"id":"_z-qCyPunITy"}},{"cell_type":"code","source":["class ReplayBuffer:\n","  def __init__(self,max_size=int(1e5), num_steps=1):\n","    self.s = np.zeros((max_size,4,84,84), dtype=np.float32)\n","    self.a = np.zeros((max_size,), dtype=np.int64)\n","    self.r = np.zeros((max_size, 1), dtype=np.float32)\n","    self.tr = np.zeros((max_size, ), dtype=np.float32)\n","    self.s_ = np.zeros((max_size,4,84,84), dtype=np.float32)\n","    self.done = np.zeros((max_size, 1), dtype=np.float32)\n","    self.ptr = 0\n","    self.size = 0\n","    self.max_size = max_size\n","    self.num_steps = num_steps\n","\n","  def append(self,s,a,r,s_,done,tr):\n","    self.s[self.ptr] = s\n","    self.a[self.ptr] = a\n","    self.r[self.ptr] = r\n","    self.tr[self.ptr] = tr\n","    self.s_[self.ptr] = s_\n","    self.done[self.ptr] = done\n","    self.ptr = (self.ptr + 1) % self.max_size\n","    self.size = min(self.size+1,self.max_size)\n","\n","  def sample(self, batch_size):\n","    probs = self.tr[:self.size] / self.tr[:self.size].sum()\n","    ind = np.random.choice(np.arange(self.size), size=batch_size, p=probs)\n","    return torch.FloatTensor(self.s[ind]),      \\\n","        torch.LongTensor(self.a[ind]),      \\\n","        torch.FloatTensor(self.r[ind]),     \\\n","        torch.FloatTensor(self.s_[ind]),      \\\n","        torch.FloatTensor(self.done[ind])"],"metadata":{"id":"I9QD7oaAnMUj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 搭建DQN神經網路的類別"],"metadata":{"id":"77N7B6M1nQe9"}},{"cell_type":"code","source":["class DQN(torch.nn.Module):\n","  def __init__(self,n_act):\n","    super(DQN,self).__init__()\n","    self.conv1 = torch.nn.Conv2d(4, 16, kernel_size=8, stride=4)  #[N,4,84,84]->[N,16,20,20]\n","    self.conv2 = torch.nn.Conv2d(16, 32, kernel_size=4, stride=2)  #[N,16,20,20]->[N,32,9,9]\n","    self.fc1 = torch.nn.Linear(32 * 9 * 9, 256)\n","    self.fc_value = torch.nn.Linear(256,32)\n","    self.fc_adv = torch.nn.Linear(256,32)\n","    self.value=torch.nn.Linear(32,1)\n","    self.adv=torch.nn.Linear(32,n_act)\n","  def forward(self,x):\n","    x = F.relu(self.conv1(x))\n","    x = F.relu(self.conv2(x))\n","    x = x.view((-1, 32 * 9 * 9))\n","    x = F.relu(self.fc1(x))\n","    value = self.value(F.relu(self.fc_value(x)))\n","    adv = self.adv(F.relu(self.fc_adv(x)))\n","    return adv + value - torch.mean(adv, dim=-1, keepdim=True)"],"metadata":{"id":"wzP-En9PnSjG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 設定是否載入模型參數，舊參數檔路徑，新參數檔路徑"],"metadata":{"id":"MSasPDsknVsL"}},{"cell_type":"code","source":["class ImageEnv(gym.Wrapper):\n","  def __init__(self,env,stack_frames=4,delay_op=50):\n","    super(ImageEnv, self).__init__(env)\n","    self.delay_op = delay_op\n","    self.stack_frames = stack_frames\n","\n","  def reset(self):\n","    s, info = self.env.reset()\n","    for i in range(self.delay_op):\n","      s, r, terminated, truncated, info = self.env.step(0)\n","      s=(s[:84, 6:90]/255.0)-0.5\n","      self.stacked_state = np.tile( s , (self.stack_frames,1,1) )  # [4, 84, 84]\n","    return self.stacked_state, info\n","\n","  def step(self, action):\n","    reward = 0\n","    for _ in range(self.stack_frames):\n","      s, r, terminated, truncated, info = self.env.step(action)\n","      s=(s[:84, 6:90]/255.0)-0.5\n","      reward += r\n","      if terminated or truncated:break\n","      self.stacked_state = np.concatenate((self.stacked_state[1:], s[np.newaxis]), axis=0)\n","    return self.stacked_state, reward, terminated, truncated, info"],"metadata":{"id":"rX3IXQE-nieA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Load_File=0\n","Old_File=f\"Model-{Load_File}.pt\"\n","\n","if Load_File>0:\n","  Log= np.load(f\"Log-{Load_File}.npy\", allow_pickle=True).item()\n","else:\n","  Log={\"TrainReward\":[],\"TestReward\":[],\"Loss\":[]}\n","\n","env=gym.make('CarRacing-v3',render_mode=\"rgb_array\",domain_randomize=False, continuous=False)\n","env = gym_wrap.GrayscaleObservation(env)\n","env = ImageEnv(env)"],"metadata":{"id":"UYnmMecQnWY9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 搭建智能體Agent的類別"],"metadata":{"id":"wAf3mKgrnZNs"}},{"cell_type":"code","source":["class DQNAgent():\n","  def __init__(self,gamma=0.9,eps_low=0.1,lr=0.00025):\n","    self.env = env\n","    self.n_act=self.env.action_space.n\n","    self.PredictDQN= DQN(self.n_act)\n","    self.TargetDQN= DQN(self.n_act)\n","    if Load_File>0:\n","      self.PredictDQN.load_state_dict(torch.load(Old_File))\n","      self.TargetDQN.load_state_dict(torch.load(Old_File))\n","    self.PredictDQN.to(device)\n","    self.TargetDQN.to(device)\n","    self.LossFun=torch.nn.SmoothL1Loss()\n","    self.optimizer=torch.optim.Adam(self.PredictDQN.parameters(),lr=lr)\n","    self.gamma=gamma\n","    self.eps_low=eps_low\n","    self.rb=ReplayBuffer(max_size=20000, num_steps=1)\n","  def PredictA(self,s):\n","    with torch.no_grad():\n","      return torch.argmax(self.PredictDQN(torch.FloatTensor(s).to(device))).item()\n","  def SelectA(self,a):\n","    return self.env.action_space.sample() if np.random.random()<self.EPS else a\n","  def Train(self,N_EPISODES):\n","    for i in tqdm(range(Load_File,N_EPISODES)):\n","      self.EPS=max(1-(i*(1-self.eps_low)/(1*N_EPISODES/10)),self.eps_low)\n","      total_reward=0\n","      step=0\n","      s,_=self.env.reset()\n","      while True:\n","        a=self.SelectA(self.PredictA(s))\n","        s_,r,done,stop,_=self.env.step(a)\n","        total_reward+=r\n","        step+=1\n","        self.rb.append(s,a,r,s_,done,step)\n","        if self.rb.size > 200 and i%self.rb.num_steps==0:self.Learn()\n","        if i % 20==0:  self.TargetDQN.load_state_dict(self.PredictDQN.state_dict())\n","        s=s_\n","        if done or stop:break\n","      print(f\"\\n{total_reward}\")\n","      Log[\"TrainReward\"].append(total_reward)\n","      if i % 10 == 9:\n","        torch.save(self.PredictDQN.state_dict(), f\"Model-{i+1}.pt\")\n","      if i % 50 == 49:\n","        test_reward=self.Test()\n","        print(f\"\\n訓練次數{i+1}，總回報{test_reward}\")\n","        Log[\"TestReward\"].append(test_reward)\n","        np.save(f\"Log-{i+1}.npy\", Log)\n","      if i % 100 == 99:\n","        display.clear_output(wait=True)\n","  def Learn(self):\n","    self.optimizer.zero_grad()\n","    batch_s, batch_a, batch_r, batch_s_, batch_done=self.rb.sample(64)\n","    predict_Q = (self.PredictDQN(batch_s.to(device))*F.one_hot(batch_a.long().to(device),self.n_act)).sum(1,keepdims=True)\n","    with torch.no_grad():\n","      target_Q = batch_r.to(device)+(1-batch_done.to(device))*self.gamma*self.TargetDQN(batch_s_.to(device)).max(1,keepdims=True)[0]\n","    loss = self.LossFun(predict_Q, target_Q)\n","    Log[\"Loss\"].append(float(loss))\n","    loss.backward()\n","    self.optimizer.step()\n","  def Test(self,VIDEO=False):\n","    total_reward=0\n","    s,_=self.env.reset()\n","    while True:\n","      a=self.PredictA(s)\n","      s,r,done,stop,_=self.env.step(a)\n","      total_reward+=r\n","      if done or stop:break\n","    return total_reward"],"metadata":{"id":"iK13qyZBnZo3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 訓練"],"metadata":{"id":"-S6Fwf32ncVs"}},{"cell_type":"code","source":["Agent=DQNAgent(gamma=0.95,eps_low=0.00,lr=0.00025)\n","Agent.Train(N_EPISODES=2500)"],"metadata":{"id":"ZZOcTYyXne-g"},"execution_count":null,"outputs":[]}]}
